{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7383cd5-ae0a-45b1-a819-de67c25f33e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16 \n",
    "from tensorflow.keras import layers, models \n",
    "from tensorflow.keras.optimizers import Adam  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddec8549-84ad-4cb4-b36a-651d363684de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "fake_dir = \"D:\\\\Revature\\\\P1\\\\temp\\\\training_fake\"\n",
    "real_dir = \"D:\\\\Revature\\\\P1\\\\temp\\\\training_real\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ff177f0-bc8a-4032-9043-d0e4b0ed380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load images from a directory and assign a label\n",
    "def load_images_from_folder(folder, label):\n",
    "    images = [] \n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename) \n",
    "        img = tf.keras.preprocessing.image.load_img(img_path, target_size=(128, 128))\n",
    "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        images.append(img_array) \n",
    "        labels.append(label)\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e768884f-1c12-4b50-8a01-551c4d9c5992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fake and real images and their corresponding labels\n",
    "fake_images, fake_labels = load_images_from_folder(fake_dir, label=0) \n",
    "real_images, real_labels = load_images_from_folder(real_dir, label=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c6d8839-06aa-4681-8e0f-fac6ce683f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split fake images into training, validation, and test sets\n",
    "X_train_fake, X_test_fake, y_train_fake, y_test_fake = train_test_split(fake_images, fake_labels, test_size=0.2, random_state=42)\n",
    "X_train_fake, X_val_fake, y_train_fake, y_val_fake = train_test_split(X_train_fake, y_train_fake, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81600a1a-ea24-4d43-aa78-805cc48cef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split real images into training, validation, and test sets\n",
    "X_train_real, X_test_real, y_train_real, y_test_real = train_test_split(real_images, real_labels, test_size=0.2, random_state=42)\n",
    "X_train_real, X_val_real, y_train_real, y_val_real = train_test_split(X_train_real, y_train_real, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43e4375c-afb3-4339-b26e-4a629294d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the training sets\n",
    "X_train = np.concatenate((X_train_fake, X_train_real), axis=0)\n",
    "y_train = np.concatenate((y_train_fake, y_train_real), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f62b28de-0bd6-4d3e-94e5-54331a32d49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the validation sets\n",
    "X_val = np.concatenate((X_val_fake, X_val_real), axis=0) \n",
    "y_val = np.concatenate((y_val_fake, y_val_real), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "693122d7-fa9e-439e-8475-cdebafb3ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the test sets\n",
    "X_test = np.concatenate((X_test_fake, X_test_real), axis=0)\n",
    "y_test = np.concatenate((y_test_fake, y_test_real), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e3e2f85-fa4f-4d01-98ab-c711b86aab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the combined training set\n",
    "train_indices = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(train_indices)\n",
    "X_train = X_train[train_indices] \n",
    "y_train = y_train[train_indices]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cffa6f6-339f-4673-b076-79eb492630f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the combined validation set\n",
    "val_indices = np.arange(X_val.shape[0])\n",
    "np.random.shuffle(val_indices)\n",
    "X_val = X_val[val_indices] \n",
    "y_val = y_val[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6514f2c-f677-4c2d-9f51-d6585f8d222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the combined test set\n",
    "test_indices = np.arange(X_test.shape[0])\n",
    "np.random.shuffle(test_indices)\n",
    "X_test = X_test[test_indices]\n",
    "y_test = y_test[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33159087-98b3-4f1c-9d71-d2a1af9376c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 1151 images\n",
      "Validation set: 289 images\n",
      "Testing set: 361 images\n"
     ]
    }
   ],
   "source": [
    "# Print dataset sizes\n",
    "print(f\"Training set: {X_train.shape[0]} images\")\n",
    "print(f\"Validation set: {X_val.shape[0]} images\") \n",
    "print(f\"Testing set: {X_test.shape[0]} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86e53681-1bc1-4229-bd5c-607531fe992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ImageDataGenerator instances\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True, \n",
    "    fill_mode='nearest' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87d58330-5ac8-49d7-bfa3-ca81b606aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datagen = ImageDataGenerator(rescale=1.0/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f79f8d3-4c33-4255-a147-3d1deeff8bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG16 model with pre-trained weights, excluding the top (fully connected) layers\n",
    "vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "628fcd53-3998-461d-97a5-8fd3e84cded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the base layers so they are not trained during fine-tuning\n",
    "vgg16_base.trainable = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e65408e-3b8f-4b7b-9a2e-0bc4885f78ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "model = models.Sequential() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3d0f863-36c2-41ce-ab7e-1b3d8ed3fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the VGG16 base model\n",
    "model.add(vgg16_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ea9c86b-2de1-4944-954f-dee446439d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))  # Regularization to prevent overfitting\n",
    "model.add(layers.Dense(1, activation='sigmoid'))  # Output layer for binary classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a13ce4a9-def8-4b5f-9f70-2b8f998b22dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "335a963c-7f9e-4032-9304-292787207c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "35/35 [==============================] - 50s 1s/step - loss: 0.7395 - accuracy: 0.5576 - val_loss: 0.6599 - val_accuracy: 0.6263\n",
      "Epoch 2/30\n",
      "35/35 [==============================] - 52s 1s/step - loss: 0.6766 - accuracy: 0.5862 - val_loss: 0.6763 - val_accuracy: 0.5709\n",
      "Epoch 3/30\n",
      "35/35 [==============================] - 52s 1s/step - loss: 0.6611 - accuracy: 0.6095 - val_loss: 0.6414 - val_accuracy: 0.6263\n",
      "Epoch 4/30\n",
      "35/35 [==============================] - 61s 2s/step - loss: 0.6466 - accuracy: 0.6152 - val_loss: 0.6445 - val_accuracy: 0.6055\n",
      "Epoch 5/30\n",
      "35/35 [==============================] - 65s 2s/step - loss: 0.6512 - accuracy: 0.6139 - val_loss: 0.6281 - val_accuracy: 0.6332\n",
      "Epoch 6/30\n",
      "35/35 [==============================] - 56s 2s/step - loss: 0.6289 - accuracy: 0.6425 - val_loss: 0.6263 - val_accuracy: 0.6263\n",
      "Epoch 7/30\n",
      "35/35 [==============================] - 58s 2s/step - loss: 0.6347 - accuracy: 0.6300 - val_loss: 0.6463 - val_accuracy: 0.6298\n",
      "Epoch 8/30\n",
      "35/35 [==============================] - 53s 2s/step - loss: 0.6249 - accuracy: 0.6542 - val_loss: 0.6342 - val_accuracy: 0.6228\n",
      "Epoch 9/30\n",
      "35/35 [==============================] - 55s 2s/step - loss: 0.6323 - accuracy: 0.6425 - val_loss: 0.6272 - val_accuracy: 0.5986\n",
      "Epoch 10/30\n",
      "35/35 [==============================] - 54s 2s/step - loss: 0.6206 - accuracy: 0.6452 - val_loss: 0.6196 - val_accuracy: 0.6159\n",
      "Epoch 11/30\n",
      "35/35 [==============================] - 56s 2s/step - loss: 0.6213 - accuracy: 0.6550 - val_loss: 0.6294 - val_accuracy: 0.6367\n",
      "Epoch 12/30\n",
      "35/35 [==============================] - 56s 2s/step - loss: 0.6137 - accuracy: 0.6631 - val_loss: 0.6212 - val_accuracy: 0.6471\n",
      "Epoch 13/30\n",
      "35/35 [==============================] - 92s 3s/step - loss: 0.6154 - accuracy: 0.6604 - val_loss: 0.6762 - val_accuracy: 0.5779\n",
      "Epoch 14/30\n",
      "35/35 [==============================] - 103s 3s/step - loss: 0.6173 - accuracy: 0.6497 - val_loss: 0.6163 - val_accuracy: 0.6574\n",
      "Epoch 15/30\n",
      "35/35 [==============================] - 99s 3s/step - loss: 0.6063 - accuracy: 0.6568 - val_loss: 0.6061 - val_accuracy: 0.6644\n",
      "Epoch 16/30\n",
      "35/35 [==============================] - 101s 3s/step - loss: 0.6177 - accuracy: 0.6586 - val_loss: 0.6170 - val_accuracy: 0.6609\n",
      "Epoch 17/30\n",
      "35/35 [==============================] - 102s 3s/step - loss: 0.6013 - accuracy: 0.6792 - val_loss: 0.5973 - val_accuracy: 0.6678\n",
      "Epoch 18/30\n",
      "35/35 [==============================] - 102s 3s/step - loss: 0.5909 - accuracy: 0.6720 - val_loss: 0.6206 - val_accuracy: 0.6609\n",
      "Epoch 19/30\n",
      "35/35 [==============================] - 101s 3s/step - loss: 0.5916 - accuracy: 0.6845 - val_loss: 0.6161 - val_accuracy: 0.6574\n",
      "Epoch 20/30\n",
      "35/35 [==============================] - 101s 3s/step - loss: 0.5937 - accuracy: 0.6765 - val_loss: 0.6434 - val_accuracy: 0.6090\n",
      "Epoch 21/30\n",
      "35/35 [==============================] - 100s 3s/step - loss: 0.5880 - accuracy: 0.6854 - val_loss: 0.5964 - val_accuracy: 0.6851\n",
      "Epoch 22/30\n",
      "35/35 [==============================] - 54s 2s/step - loss: 0.5843 - accuracy: 0.6908 - val_loss: 0.5896 - val_accuracy: 0.6747\n",
      "Epoch 23/30\n",
      "35/35 [==============================] - 52s 1s/step - loss: 0.5879 - accuracy: 0.6774 - val_loss: 0.6313 - val_accuracy: 0.6332\n",
      "Epoch 24/30\n",
      "35/35 [==============================] - 52s 1s/step - loss: 0.5925 - accuracy: 0.6890 - val_loss: 0.5928 - val_accuracy: 0.6851\n",
      "Epoch 25/30\n",
      "35/35 [==============================] - 52s 1s/step - loss: 0.5912 - accuracy: 0.6810 - val_loss: 0.6239 - val_accuracy: 0.6263\n",
      "Epoch 26/30\n",
      "35/35 [==============================] - 52s 1s/step - loss: 0.5903 - accuracy: 0.6643 - val_loss: 0.6006 - val_accuracy: 0.6436\n",
      "Epoch 27/30\n",
      "35/35 [==============================] - 54s 2s/step - loss: 0.5692 - accuracy: 0.7042 - val_loss: 0.6249 - val_accuracy: 0.6332\n",
      "Epoch 28/30\n",
      "35/35 [==============================] - 57s 2s/step - loss: 0.5767 - accuracy: 0.7060 - val_loss: 0.5877 - val_accuracy: 0.6713\n",
      "Epoch 29/30\n",
      "35/35 [==============================] - 54s 2s/step - loss: 0.5831 - accuracy: 0.6845 - val_loss: 0.6063 - val_accuracy: 0.6505\n",
      "Epoch 30/30\n",
      "35/35 [==============================] - 54s 2s/step - loss: 0.5537 - accuracy: 0.6988 - val_loss: 0.5867 - val_accuracy: 0.6713\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit( \n",
    "    train_datagen.flow(X_train, y_train, batch_size=32),\n",
    "    steps_per_epoch=len(X_train) // 32,\n",
    "    validation_data=val_datagen.flow(X_val, y_val, batch_size=32),\n",
    "    epochs=30\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14e4a871-1df4-484e-8cab-5d84106c11ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 13s 1s/step - loss: 0.6575 - accuracy: 0.6122\n",
      "Test Accuracy: 0.6122\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(val_datagen.flow(X_test, y_test, batch_size=32))\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "617ce85d-99d0-4ee6-81eb-30e6e3694f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the last few layers of VGG16\n",
    "vgg16_base.trainable = True\n",
    "for layer in vgg16_base.layers[:-4]:  # Unfreeze the last 4 layers\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8630f7b9-fa4c-422f-9e0c-333317c3f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-compile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d30aa40e-8078-463f-aa4c-a65af511eda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "35/35 [==============================] - 67s 2s/step - loss: 0.5673 - accuracy: 0.6872 - val_loss: 0.5973 - val_accuracy: 0.6471\n",
      "Epoch 2/10\n",
      "35/35 [==============================] - 67s 2s/step - loss: 0.5562 - accuracy: 0.7122 - val_loss: 0.6243 - val_accuracy: 0.6263\n",
      "Epoch 3/10\n",
      "35/35 [==============================] - 67s 2s/step - loss: 0.5383 - accuracy: 0.7373 - val_loss: 0.5943 - val_accuracy: 0.6990\n",
      "Epoch 4/10\n",
      "35/35 [==============================] - 66s 2s/step - loss: 0.5376 - accuracy: 0.7283 - val_loss: 0.6036 - val_accuracy: 0.6747\n",
      "Epoch 5/10\n",
      "35/35 [==============================] - 69s 2s/step - loss: 0.5384 - accuracy: 0.7122 - val_loss: 0.6196 - val_accuracy: 0.6644\n",
      "Epoch 6/10\n",
      "35/35 [==============================] - 67s 2s/step - loss: 0.5350 - accuracy: 0.7310 - val_loss: 0.6019 - val_accuracy: 0.6609\n",
      "Epoch 7/10\n",
      "35/35 [==============================] - 68s 2s/step - loss: 0.5032 - accuracy: 0.7534 - val_loss: 0.6201 - val_accuracy: 0.6609\n",
      "Epoch 8/10\n",
      "35/35 [==============================] - 67s 2s/step - loss: 0.4840 - accuracy: 0.7676 - val_loss: 0.6029 - val_accuracy: 0.6713\n",
      "Epoch 9/10\n",
      "35/35 [==============================] - 67s 2s/step - loss: 0.4887 - accuracy: 0.7569 - val_loss: 0.6000 - val_accuracy: 0.6678\n",
      "Epoch 10/10\n",
      "35/35 [==============================] - 67s 2s/step - loss: 0.4856 - accuracy: 0.7641 - val_loss: 0.6246 - val_accuracy: 0.6574\n"
     ]
    }
   ],
   "source": [
    "# Continue training \n",
    "history_fine = model.fit( \n",
    "    train_datagen.flow(X_train, y_train, batch_size=32),\n",
    "    steps_per_epoch=len(X_train) ,\n",
    "    validation_data=val_datagen.flow(X_val, y_val, batch_size=32),\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d1a65b5-fe66-4085-ad44-4219ad84ad11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 13s 1s/step - loss: 0.7322 - accuracy: 0.6039\n",
      "Test Accuracy after fine-tuning: 0.6039\n"
     ]
    }
   ],
   "source": [
    "# Evaluate again\n",
    "test_loss, test_acc = model.evaluate(val_datagen.flow(X_test, y_test, batch_size=32))\n",
    "print(f\"Test Accuracy after fine-tuning: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cec668e-8c64-45db-b2c0-361b9faf1d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"D:\\\\Revature\\\\P1\\\\models\\\\my_vgg16_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0086933-3a86-428d-b952-b417e656de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the image you want to predict\n",
    "image_path = r\"D:\\Revature\\P1\\28-8-24\\training_fake\\easy_38_0010.jpg\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
